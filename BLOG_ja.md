# 概要
メイン解法の技術的な話は [SOLUTION_ja.md](SOLUTION_ja.md) （いずれ書く？）に任せて、ここでは解法に至った道筋やコンテストの間にやったことを時系列で追っていく。
ちなみに今これを書いているのは最終日の 15:17 であり、そこそこ時間がないなかでの執筆となっているため誤字脱字は大目に見て欲しい。（全て温かみのある手書きである。）

# ヒューリスティックコンテストと機械学習
ヒューリスティックコンテストと機械学習は案外相性が良い。詳しくは nagiss さんの[ヒューリスティックコンテストで機械学習しよう](https://speakerdeck.com/nagiss/hiyurisuteitukukontesutodeji-jie-xue-xi-siyou)を参照してもらいたいが、例えば[AHC016 Graphorean](https://atcoder.jp/contests/ahc016/tasks/ahc016_a) ではその nagiss さんが transformer で圧倒的1位を取っている（[解説](https://engineering.dena.com/blog/2022/12/httf-2023-qual/)）。最近ではニューラルネットワーク（Neural Network, NN）でハイパーパラメータ調整を行っている人もいた気がする。

しかし、同資料内で指摘のある通り、コンテスト中に強化学習（Reinforcement Learning, LR）を end-to-end で用いた解法を成功させて勝ったことのある人は未だに存在しない。たまに相性の良さそうな問題が出題されても、探索ベースの手法が強すぎるためか、あるいは AtCoder の提出サイズ制限は 512 KiB と小さめなせいか、1ページ目に RL 解法入っているのは見たことがない。

その一方で、延長線順位表でならば RL で1位を取ったことのある人はいる。[AHC015](https://atcoder.jp/contests/ahc015/tasks/ahc015_a)（Halloween Candy）の saitodevel01 さんは[提出](https://atcoder.jp/contests/ahc015/submissions/58279157)（本人に確認を取ったわけではないが）おそらく RL をしていそうである。そのため、短期AHCは無理だとしても、長期AHCでならば RL で優勝できるのではないかと数年ほど前から考えていた。毎長期AHCで RL 解法を考えると同時に、逆に writer として [RL で勝てる問題](https://atcoder.jp/contests/ahc-survey2024#:~:text=AHC%E3%81%AB%E3%81%93%E3%82%8C%E3%81%BE%E3%81%A7%E3%81%82%E3%81%BE%E3%82%8A%E5%87%BA%E9%A1%8C%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%81%AA%E3%81%84%E3%82%B8%E3%83%A3%E3%83%B3%E3%83%AB%E3%83%BB%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF%E3%82%92%E4%BD%BF%E3%81%86%E5%95%8F%E9%A1%8C%E3%80%82)を出すのも面白いのではないかと思い原案候補を思いついては一人で棄却するAHC生活を続けていたが、めでたく AHC061 で優勝できそうなため、多分 writer をやるのはもっと先になりそう。（そもそも懇親会等において RL で AHC 勝ちたいっすね〜というような内容をことあるごとに話していたため、そのような解法が強い問題を出題するべきではないという話もある。）


# 時系列順の振り返り

## 2/13(金) 1日目

問題を読むと、一目で強化学習（Reinforcement Learning, RL）で勝てそうな見た目をしていることに気づく。 問題の構造としては先程の AHC015 が近く、主な違いは隠れパラメータの推定が必要となることのみである。隠れパラメータの推定を真面目にやるならばガウス分布を用いた近似や粒子フィルターを用いるのだろうが、前者は近似の導入による精度の悪化、後者は速度面に難がある。一方で RL ならば陽にパラメータ推定を行わなくても良いため、AHC061 は AHC015 よりも RL 向きだと思い、初日の時点で RL 1本で勝負する選択をした。

## 2/14(土) 2日目

土曜日なのでがっつり RL 用のコードの整備を始める。実行時間制限が短く提出コード内で探索を行う余裕がなく policy をそのまま予測したいため、PPO を使うことにする。この時点ではアーキテクチャーは全く拘っておらず、プレイヤーと各 AI の特徴量を channel 方向に concat した上で 64 channels + 16 blocks（c64b16）の Resnet を使っている。一応軽量化のために単純な 3x3 convolution ではなく depthwise seperable convolution を使いパラメータ数を削減している。policy は 10x10 で駒の移動先のマスの確率分布で指定しており、学習中は確率分布からサンプリング、評価時は最も確率の高いマスを指定するようにする。また、プレイヤー0以外のプレイヤーはスコア順にソートしてから NN に渡すことで、現在の局面における有効手を学習させやすくしている。

数時間コードを回したらなんかいい感じに学習が進んだため、サブしたところ[瞬間1位](https://x.com/ethylene_66/status/2022588584105300301)を取る。この時 RL で勝てることを確信する。

## 2/15(日) 3日目

日曜日なので PC に向かいコードを書く。そろそろ報酬をグラフで見たり実験管理を真面目にしたくなってくる頃なので、wandb を使えるようにする。また、新しい特徴量を追加したり、戦略をより大きいモデルに蒸留できるようにコードを整備する。

NNは大きいモデルほど容量が大きく複雑な戦略を学習できる一方、小さいモデルの方が1ステップあたりの学習が速く序盤は性能の改善速度が速い傾向がある。そこで、RL を回している中である程度性能がサチったら、少し大きいサイズのモデルに蒸留し RL を再開する、といった戦略がコスパが良い。（小モデル→大モデルの場合も蒸留と言うのかは自信がない。）そこで、サチり気味だった c64b16 を用い c128b16 を蒸留し RL を回し、サブするときはまた c64b16 に蒸留し直す、ということを行うことにする。

その他学習の高速化を行う。

## 2/16(月) 4日目

あまり時間が取れないのもあり、圧縮アルゴリズムや推定器の簡単な改善や、学習の高速化のみ行う。また他のアーキテクチャーも試すが、まだ良さそうなものは見つからない。

隠しパラメータの推定器として今までは粒子フィルターを用いていたが、粒子数が少ないと精度が悪い一方で粒子数を増やすと速度が壊滅的に遅くなるという欠点があった。RL を行うなら多少推定値が間違っていようとモデルがなんとかしてくれるだろうという読みでより高速な手法を色々試したところ、AHCで何度もお世話になっているガウス分布で近似することにする。詳しくは exps/exp003 を参照してほしい。

## 2/17(火) 5日目

このあたりから計算資源不足に悩まされることになる。ローカルの RTX 5090 を回していたが、RL は計算資源があればあるほど良いので計算資源を増やしたくなる。丁度 Runpod に $150 ほどクレジットが余っていたため、これを使用して RTX 5090 * 6 のサーバーを立てる。これにより学習が 4~5 倍ほど速くなり QOL が改善する。

他には、推定器に small update を入れて改善する。


## 2/18(水) 6日目

c128 b16 でも学習がサチってきたため、新しいアーキテクチャーの模索を再開する。今のアーキテクチャーでは全てプレイヤーの生の特徴量をそのまま concat してから1つの Resnet に突っ込んでおり、個々のプレイヤーの生の特徴量から意味のある特徴量を抽出するのが難しそうである。そこで、プレイヤー0とプレイヤー$i$（$1 \leq i < M$）の生の特徴量を concat した入力のみを受け取る浅めのネットワークを新たに追加し、その出力を Resnet の真ん中に混ぜることにする。つまり、1回の推論においてこの新しいネットワークにはプレイヤー $(0, 1), (0, 2), ..., (0, M-1)$ の $M-1$ 個のデータが流れる。プレイヤー$i$（$1 \leq i < M$）とプレイヤー$0$を同時に流しているのは、その間の相互作用が最も大事そうだからである。

このアーキテクチャーは成功を収めたものの、学習速度が大幅に下がってしまった。そこで丁度 runpod のクレジットが無くなりそうということもあり vast.ai に新たに RTX 5090 * 8 のサーバーを2つ立てて RL を回し始めた。ちなみに片方のサーバーは現時点（2/23 17:17）も起動しっぱなしである。

## 2/19(木) 7日目

サブ用モデルの改善を図る。今までは c64b16 をサブに使う予定だったが、色々頑張ることで新しいアーキテクチャーの c112b16 ならサブできそうということに気づく。具体的には、base91 encoding ハフマン符号化を組み合わせ、さらにモデルの重みを float32 で持つのではなく int8 と float16 で埋め込むことでギリギリ 512 KiB に収まる。

このあたりでまた RL がサチり始める。実験したところ学習率（learning rate; lr）と entropy coefficient が大きすぎるらしい。しかし、それらを一気に下げると学習が全く進まなくなったため、両方 -20% 程度小さくして RL を再開する。今後学習がサチる度にこの操作を行っている。

## 2/20(金) 8日目

再び RL がサチってしまった。今度は lr ろ entropy coefficient を下げても学習が進まない。そこで、性能に対してモデルサイズが小さすぎるのだろうと思い、Resnet の 3x3 conv を全て depthwise seperable convolution から通常の 2d convolution に戻し学習を再開すると、今度はちゃんと報酬が上がり始める。

その過程において再度新しいアーキテクチャーを探索したものの、他に良いものは見つからなかった。個人的には attention でプレイヤー間の相互作用を学習させたかったが、学習がうまく進まなかった。

## 2/21(土) 9日目

RL しているメインのモデルからサブ用のモデルに蒸留する際に性能が悪化していそうなのを発見する。無駄なコードを削除したり変数名を置換することで c112b18 まで埋め込めるようになる。さらに、蒸留時のハイパーパラメータ探索も行い、蒸留による性能劣化を1%未満に抑える。

## 2/22(日) 10日目

もう特にやることもないので、wandb のグラフを眺めながらサブ用モデルの蒸留を行い違う条件での性能差を測定する。

# 振り返り

RL の良い勉強になった。PPO の実装方法だけでなく、細かいパラメータの学習に与える影響についての勘所も多少は養われた気がする。今後も機会があれば AHC で機械学習をやっていきたい。

最後に、面白い問題を出題してくださった writer の Shun_PI さん及びスポンサーのTHIRD社さんありがとうございます。

# サーバー

| クラウド | GPU数 | 時間 | GPU時間 | 料金 |
| --- | --- | --- | --- | --- |
| runpod | 6 gpus | 26 hours | 156 gpu hours | $141 |
| vast.ai | 8 gpus | 138 hours | 1104 gpu hours | $457 |

これに加え、自宅の RTX 5090 を常時回している。

（あれ、結構かかってるな......1位の賞金より高くない？）

